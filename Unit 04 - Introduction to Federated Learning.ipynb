{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb1da77",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "Federated learning is a machine learning paradigm that enables decentralized training of a shared model by multiple clients while preserving data privacy. The main idea behind this new paradigm is that each client trains a local model on its own data and then sends only the model updates to a central server, rather than sending the raw data. This allows the model to be trained on a large amount of data without compromising data privacy.\n",
    "\n",
    "Federated learning was first proposed by Google in 2016 (McMahan et al., 2016) and has since been applied in various fields, such as healthcare (Hard et al., 2018), finance (Yoon et al., 2018), and natural language processing (Li et al., 2020).  For example, federated learning could be used to train a model that can make personalized recommendations for each user without requiring the raw data from each user to be shared with a central server. This is the mechanism by which the model is trained on data, while adhering to data privacy requirements.\n",
    "\n",
    "Federated learning is a machine learning approach that offers numerous advantages over traditional centralized methods. Firstly, by leveraging distributed data stores, federated learning can scale to handle significantly larger datasets. Secondly, it prioritizes data privacy by avoiding the transmission of raw data to a central server. Finally, federated learning enables collaboration among multiple clients, allowing them to jointly train a shared model without compromising the security of their individual data. Overall, these benefits make federated learning a promising approach for machine learning in fields where data privacy is of the utmost importance.\n",
    "\n",
    "Federated learning is a process in which a central server distributes a machine learning model to multiple devices. Each device trains the model on its local data and sends the updated model back to the central server. The central server then aggregates the updates from each device to improve the global model. This process is repeated until the model converges and can generate accurate predictions on new data. The key concepts within this process are:\n",
    "\n",
    "\n",
    "* Client: refers to a device or edge node that holds a local dataset and actively participates in the training of the federated model.\n",
    "* Server: represents the central entity that coordinates the training of the federated model and receives model updates from the clients to aggregate into a new version of the global model.\n",
    "* Federated dataset: the collection of decentralized datasets from different clients that are used to train the federated model through collaborative learning.\n",
    "* Federated model: a machine learning model that is trained on the federated dataset using federated learning to make accurate predictions on new data while preserving the privacy of each client's data.\n",
    "* Federated optimization: refers to the process of training the federated model using the decentralized data and model updates from the clients, which enables the model to generalize better on unseen data while preserving the privacy of the clients.\n",
    "* Aggregation: the process of combining the model updates received from the clients into a new version of the global model. This can be done using various methods such as weighted averaging or other approaches.\n",
    "* Rounds: refer to the number of times a federated model is distributed among clients after performing an aggregation to train the model further. The process is repeated until the model converges and achieves a satisfactory level of accuracy.\n",
    "\n",
    "\n",
    "Federated learning is a relatively new approach, and as such, there are few libraries available that have adapted to it. The main actors in this space are TensorFlow Federated, PySyft, OpenMined, and Flower. Of these, TensorFlow Federated is a notable mention, although it is currently only a theoretical approach, as it does not allow for the deployment of the solution and only simulates the federated space. In contrast, Flower allows for the distribution of federated learning, although the necessary modifications can be somewhat challenging. For this tutorial, we have chosen Flower due to its more user-friendly approach and potential for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf254d5",
   "metadata": {},
   "source": [
    "# Introduction to Flower (FLWR)\n",
    "\n",
    "Flower is a Python library that offers tools for implementing the communication and coordination aspects of federated learning. Its design emphasizes ease of use and scalability. It's important to note that Flower is not a learning framework in itself, and as such, it wraps other machine learning frameworks like TensorFlow, PyTorch, or Scikit-learn in the communication layer to enable federated learning.\n",
    "\n",
    "To use Flower for federated learning, you will need to install the library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2f2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 12:37:15.574017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 12:37:15.574068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 12:37:15.574952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-13 12:37:15.580480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-13 12:37:16.251339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import flwr as fl\n",
    "except ImportError as err:\n",
    "    !pip install flwr[simulation]\n",
    "    \n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as err:\n",
    "    !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ce39",
   "metadata": {},
   "source": [
    "When setting up a simulation environment, it's best to use the *simulation* keyword with the command to ensure the appropriate environment is loaded. On the other hand, if you plan to use Flower in a distributed setup, the command should be `!pip install flwr` on both the server and client devices. After installing the `flwr` package, you can import it into your Python code using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b68fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a157b7b",
   "metadata": {},
   "source": [
    "FLWR provides a range of classes and functions that you can use to set up a federated learning environment, train and evaluate a model, and implement regular updates to the model. You can refer to the FLWR [documentation](https://flower.dev/docs/quickstart-tensorflow.html)  for more information. Before proceeding, it's important to note that the model you define must be serializable so that it can be sent through the network. Not all models are suitable for federated learning. For this example, we'll be using an Artificial Neural Network (ANN) based on TensorFlow, specifically Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd89deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model using TensorFlow\n",
    "# def generate_ann():\n",
    "#     model = tf.keras.Sequential(\n",
    "#         [\n",
    "#             tf.keras.layers.Flatten(input_shape=(32, 32, 3)), #input data. 32x32 color images with 3 channels\n",
    "#             tf.keras.layers.Dense(64, activation=\"relu\"), #hidden layer\n",
    "#             tf.keras.layers.Dense(64, activation=\"relu\"),#hidden layer\n",
    "#             tf.keras.layers.Dense(10, activation=\"softmax\"), #output layer. 10 classes\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "#         optimizer=tf.keras.optimizers.Adam(),\n",
    "#         metrics=[\"accuracy\"],\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "def generate_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167daad",
   "metadata": {},
   "source": [
    "As we will be using a Deep Learning model defined in TensorFlow, it's recommended to load the data into a `Dataset` class to enable the framework to leverage any available hardware acceleration (such as a GPU on the nodes). However, due to some limitations of the framework in order to serialize the data, it has to be done manualy with the following lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9b84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10000], y_train[:10000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into NUM_CLIENTS partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754d7f5",
   "metadata": {},
   "source": [
    "Before continuing, it is a good Idea to make some test in order to check the correctness of our approach which could deal with this. For example, training a model on the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4fe1172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 81ms/step - loss: 2.3260 - accuracy: 0.0729\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.3006 - accuracy: 0.1350\n",
      "The resulting loss is 2.3006362915039062 for an accuracy in test of 1.3500%\n"
     ]
    }
   ],
   "source": [
    "model = generate_ann()\n",
    "\n",
    "model.fit(trainloaders[0][0],trainloaders[0][1], epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "loss, accuracy = model.evaluate(valloaders[0][0], valloaders[0][1])\n",
    "print(f\"The resulting loss is {loss} for an accuracy in test of {accuracy*10:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c70c",
   "metadata": {},
   "source": [
    "**Important**: *The following example is thought to be executed in a terminal* \n",
    "\n",
    "Starting from that point, that it works on a particular data, the federated processs is going to split this process in several machines and for that it is going to require de definition of a couple of additional elements. Therefore, let's introduce the two pieces of the puzzle: the `Client` and the `Server`. \n",
    "\n",
    "\n",
    "\n",
    "Flower starts a `Server` to coordinate the client devices and perform the orchestration of the model. The server interacts with clients through an interface called `Client`. When the server selects a particular client for training, it sends training instructions over the network. The client receives those instructions and calls one of the Client methods to run your code, which in this case involves training the neural network that we defined earlier.\n",
    "\n",
    "Flower provides a convenient class called NumPyClient, which simplifies the implementation of the Client interface when your workload uses Keras. The [NumPyClient](https://flower.ai/docs/framework/ref-api/flwr.client.NumPyClient.html#flwr.client.NumPyClient) interface defines three methods that can be implemented in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bde54e",
   "metadata": {},
   "source": [
    "```python\n",
    "#Create a class to contain the details of the client and be the interface\n",
    "class MyClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, train_dataset, test_dataset):\n",
    "        self.model = net\n",
    "        self.trainloader = train_dataset\n",
    "        self.valloader = test_dataset\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.trainloader[0],self.trainloader[1], epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "        return self.model.get_weights(), len(self.trainloader[0]), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.valloader[0], self.valloader[1])\n",
    "        return loss, len(self.valloader[0]), {\"accuracy\": float(accuracy)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324ea6a",
   "metadata": {},
   "source": [
    "In the preceding code, we defined the required functions for the client in this particular case. With these functions in place, we can now start a client using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e187591",
   "metadata": {},
   "source": [
    "```python \n",
    "# Start the client\n",
    "model=generate_ann()\n",
    "\n",
    "#The following line is deprecated in the current flower version\n",
    "# fl.client.start_numpy_client(server_address=\"localhost:8080\", client=MyClient(model,trainloaders[0],valloaders[0]))\n",
    "\n",
    "fl.client.start_client(server_address=f\"localhost:8080\", client=MyClient(model,trainloaders[0],valloaders[0]).to_client())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d96b24",
   "metadata": {},
   "source": [
    "**Important**: In order to run the client you must need also a server running!!! You will executed both in separated terminals\n",
    "\n",
    "The string`localhost:8080` specifies the server to which the client should connect. In this case, as the code is being run on the same machine as the server, this address is sufficient. In a truly federated workload, the only thing that needs to be changed is the `server_address` to point the client to the correct server.\n",
    "\n",
    "Note that Jupyter usually runs on port 8080, **so you will need to use another available port if Jupyter server is running**.\n",
    "\n",
    "~~Possible exception: if you get the following error: \"failed to connect to all addresses\" the you shoul use the following string to stablish the connection `localhost:8080` instead of `[::]:8080`~~\n",
    "\n",
    "\n",
    "\n",
    "The other essential piece of the puzzle is the class that will contain the server. This will be in a separate file, for example server.py, and its contents should look something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb282a",
   "metadata": {},
   "source": [
    "```python\n",
    "import flwr as fl\n",
    "\n",
    "fl.server.start_server(config=fl.server.ServerConfig(num_rounds=3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684abf1",
   "metadata": {},
   "source": [
    "##### **Important**\n",
    "You can use another port for running the server if you fix the following parameter in the `start_server` function: `server_address=\"localhost:8080\"`\n",
    "\n",
    "In this particular case, we can run two clients and a server in separate terminals of the machine. Running two client instances is as simple as executing the `python client.py` command twice in separate terminals, while the server can be started with the `python server.py` command.\n",
    "\n",
    "Upon starting the server, we should receive an output similar to:\n",
    "\n",
    "\n",
    "```shell\n",
    "INFO flwr 2023-03-01 14:58:16,353 | app.py:139 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
    "INFO flwr 2023-03-01 14:58:16,362 | app.py:152 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:86 | Initializing global parameters\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:270 | Requesting initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,152 | server.py:274 | Received initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:88 | Evaluating initial parameters\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:101 | FL starting\n",
    "DEBUG flwr 2023-03-01 14:58:26,118 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,041 | server.py:229 | fit_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,076 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,076 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,565 | server.py:179 | evaluate_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,565 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,566 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,015 | server.py:229 | fit_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,027 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:179 | evaluate_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,755 | server.py:229 | fit_round 3 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,769 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:29,184 | server.py:179 | evaluate_round 3 received 2 results and 0 failures\n",
    "INFO flwr 2023-03-01 14:58:29,185 | server.py:144 | FL finished in 5.031599427999936\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:202 | app_fit: losses_distributed [(1, 2.3956351280212402), (2, 2.426431179046631), (3, 2.3015435934066772)]\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:203 | app_fit: metrics_distributed {}\n",
    "INFO flwr 2023-03-01 14:58:29,186 | app.py:204 | app_fit: losses_centralized []\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8f5a6",
   "metadata": {},
   "source": [
    "With that, the first federated learning approach is completed. As you can see, the system goes through three rounds of fitting and evaluating on all clients before the results are retrieved, aggregated, and redistributed to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00137c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Implement the client and server code in separate files (**All the required files must be submitted together with the Notebook**). Next, execute a server and two clients from terminals. Finally, compare the results with those presented here. Were your results similar?\n",
    "\n",
    "\n",
    "**Note**:  All the required code to execute the server as well as the clients can be found this notebook. Considering the output displayed in the preceding cell, the following answer is expected to contain the corresponding terminal output resulting from the execution of your code.\n",
    "\n",
    "`Answer here`:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ffe9d-3c1d-41ca-a072-02374f9bf5d9",
   "metadata": {},
   "source": [
    "```shell\n",
    "INFO flwr 2024-03-12 14:20:02,531 | app.py:163 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
    "INFO flwr 2024-03-12 14:20:02,554 | app.py:176 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
    "INFO flwr 2024-03-12 14:20:02,554 | server.py:89 | Initializing global parameters\n",
    "INFO flwr 2024-03-12 14:20:02,554 | server.py:276 | Requesting initial parameters from one random client\n",
    "INFO flwr 2024-03-12 14:20:41,589 | server.py:280 | Received initial parameters from one random client\n",
    "INFO flwr 2024-03-12 14:20:41,590 | server.py:91 | Evaluating initial parameters\n",
    "INFO flwr 2024-03-12 14:20:41,590 | server.py:104 | FL starting\n",
    "DEBUG flwr 2024-03-12 14:21:00,205 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:01,502 | server.py:236 | fit_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2024-03-12 14:21:01,508 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2024-03-12 14:21:01,508 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:02,098 | server.py:187 | evaluate_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2024-03-12 14:21:02,098 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2024-03-12 14:21:02,099 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:02,543 | server.py:236 | fit_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2024-03-12 14:21:02,548 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:03,076 | server.py:187 | evaluate_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2024-03-12 14:21:03,076 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:03,539 | server.py:236 | fit_round 3 received 2 results and 0 failures\n",
    "DEBUG flwr 2024-03-12 14:21:03,545 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2024-03-12 14:21:03,967 | server.py:187 | evaluate_round 3 received 2 results and 0 failures\n",
    "INFO flwr 2024-03-12 14:21:03,968 | server.py:153 | FL finished in 22.377772565003397\n",
    "INFO flwr 2024-03-12 14:21:03,968 | app.py:226 | app_fit: losses_distributed [(1, 2.39928936958313), (2, 2.337975263595581), (3, 2.342345952987671)]\n",
    "INFO flwr 2024-03-12 14:21:03,968 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
    "INFO flwr 2024-03-12 14:21:03,968 | app.py:228 | app_fit: metrics_distributed {}\n",
    "INFO flwr 2024-03-12 14:21:03,968 | app.py:229 | app_fit: losses_centralized []\n",
    "INFO flwr 2024-03-12 14:21:03,968 | app.py:230 | app_fit: metrics_centralized {}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25335",
   "metadata": {},
   "source": [
    "# Updating parameters\n",
    "The key element in this kind of approach is that the server sends the global model parameters to the client, and the client updates the local model with the parameters received from the server. It then trains the model on the local data, which changes the model parameters locally. After training, the updated model parameters are sent back to the server, or alternatively, only the gradients are sent back to the server, not the full model parameters.\n",
    "\n",
    "\n",
    "In `flwr`, this communication is essentially done by two helper functions for loading and retrieving local parameters: `set_parameters` and `get_parameters`. This requirement fits well with non-state approaches such as **PyTorch** or **JAX**. As demonstrated in the previous example, `flwr` can also be used with **TensorFlow** or even **scikit-learn**.\n",
    "\n",
    "As a result, the basic structure for any client using this library has the same format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "467be0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Utility functions for the most common operations\n",
    "def get_parameters(net) -> List[np.array]:\n",
    "    return net.get_weights()\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    net.set_weights(parameters)\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    net.fit(trainloader[0], trainloader[1],\n",
    "            epochs=epochs, batch_size=32, steps_per_epoch=3)\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    loss, accuracy = net.evaluate(testloader[0], testloader[1])\n",
    "    return loss, accuracy\n",
    "\n",
    "# Class to contain a Client\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        self.net = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"[Client {self.cid}] loss:{loss}, Client {self.cid} accuracy:{accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e352",
   "metadata": {},
   "source": [
    "In Flower, clients can be created by extending either the `flwr.client.Client` or `flwr.client.NumPyClient` classes. In the previous example, we used `NumPyClient` because it is easier to implement and requires less code as a template. Along with the extended class, there are three main methods that need to be implemented:\n",
    "\n",
    "* `get_parameters`: Returns the current local model parameters.\n",
    "* `fit`: Receives model parameters from the server, trains the model parameters on the local data, and returns the (updated) model parameters to the server.\n",
    "* `evaluate`: Receives model parameters from the server, evaluates the model parameters on the local data, and returns the evaluation result to the server.\n",
    "\n",
    "As you can see, the `MyClient` class implemented in the previous example follows this same structure and the diference is the *id* of the client which is stored for later convinience use in accesing the data.\n",
    "\n",
    "\n",
    "#### Be aware: \n",
    "Sometimes, especially when we are simulating multiple clients on a single device, it can be useful to use a function to create the client when it is required. This is particularly important in stateless frameworks, such as PyTorch, which can benefit from a more efficient implementation that creates clients only when they are required for training or evaluation. For example, the following code loads different examples for each client before discarding them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2bc239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid) -> FlowerClient:\n",
    "    # Create the model\n",
    "    net = generate_ann()\n",
    "    #Take the appropiate part of the dataset\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    #Create and return the Client\n",
    "    return FlowerClient(cid, net, trainloader, valloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb598",
   "metadata": {},
   "source": [
    "Note that `myClient` cannot be used in the same sense because of the state that it keeps internally through the function `generate_ann`. However, if this state is removed, it can be used in the same way.\n",
    "\n",
    "The clients are now set up to load, fit, and evaluate. However, we need to integrate the results from the different clients. In Flower terminology, this is known as a strategy, such as the *Federated Average (FedAvg)* strategy. In a first approach, we can use the built-in implementations of the framework, although custom strategies can also be used. Let's see an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723d5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-13 12:53:45,339 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2024-03-13 12:53:49,892\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-03-13 12:53:51,094 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 4104285390.0, 'node:172.27.123.152': 1.0, 'object_store_memory': 2052142694.0}\n",
      "INFO flwr 2024-03-13 12:53:51,095 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-03-13 12:53:51,095 | app.py:227 | No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2024-03-13 12:53:51,096 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-03-13 12:53:51,110 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 12 actors\n",
      "INFO flwr 2024-03-13 12:53:51,111 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-03-13 12:53:51,112 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2024-03-13 12:53:51,114 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2024-03-13 12:53:51,115 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-03-13 12:53:51,117 | server.py:222 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m 2024-03-13 12:53:53.320835: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m 2024-03-13 12:53:53.320874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m 2024-03-13 12:53:53.333552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=16462)\u001b[0m 2024-03-13 12:53:53.349309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=16462)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m 2024-03-13 12:53:55.531813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "\u001b[2m\u001b[36m(pid=16456)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m 2024-03-13 12:53:58.841559: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m 2024-03-13 12:53:53.320838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m 2024-03-13 12:53:53.320878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m 2024-03-13 12:53:53.333551: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16463)\u001b[0m 2024-03-13 12:53:53.350325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16463)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:53:59,612 | server.py:236 | fit_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2024-03-13 12:53:59,622 | fedavg.py:250 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-03-13 12:53:59,623 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 1s - loss: 2.3292 - accuracy: 0.1250\n",
      "3/3 [==============================] - 1s 8ms/step - loss: 2.4681 - accuracy: 0.1146\n",
      "3/3 [==============================] - 1s 8ms/step - loss: 2.5738 - accuracy: 0.1146\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m [Client 2] evaluate, config: {}\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3298 - accuracy: 0.0800\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m [Client 2] loss:2.329796075820923, Client 2 accuracy:0.07999999821186066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:54:00,462 | server.py:187 | evaluate_round 1 received 2 results and 0 failures\n",
      "WARNING flwr 2024-03-13 12:54:00,463 | fedavg.py:281 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-03-13 12:54:00,464 | server.py:222 | fit_round 2: strategy sampled 5 clients (out of 5)\n",
      "DEBUG flwr 2024-03-13 12:54:02,906 | server.py:236 | fit_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:54:02,919 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 5)\n",
      "ERROR flwr 2024-03-13 12:54:03,919 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 12:54:03,920 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-13 12:54:03,921 | server.py:187 | evaluate_round 2 received 1 results and 1 failures\n",
      "DEBUG flwr 2024-03-13 12:54:03,922 | server.py:222 | fit_round 3: strategy sampled 5 clients (out of 5)\n",
      "ERROR flwr 2024-03-13 12:54:04,095 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step - loss: 2.2984 - accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-13 12:54:04,214 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=16466)\u001b[0m [Client 0] fit, config: {}\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "1/3 [=========>....................] - ETA: 1s - loss: 2.3919 - accuracy: 0.0312\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.3493 - accuracy: 0.0729\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 2.4389 - accuracy: 0.0833\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16466)\u001b[0m [Client 0] evaluate, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3164 - accuracy: 0.1000\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16466)\u001b[0m [Client 0] loss:2.298365831375122, Client 0 accuracy:0.125\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-13 12:54:05,361 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 12:54:05,363 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-13 12:54:05,920 | server.py:236 | fit_round 3 received 3 results and 2 failures\n",
      "DEBUG flwr 2024-03-13 12:54:05,924 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 5)\n",
      "DEBUG flwr 2024-03-13 12:54:06,524 | server.py:187 | evaluate_round 3 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:54:06,525 | server.py:222 | fit_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 0s - loss: 2.2952 - accuracy: 0.1250\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2879 - accuracy: 0.1250\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2669 - accuracy: 0.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-13 12:54:07,532 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 12:54:07,533 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-13 12:54:08,025 | server.py:236 | fit_round 4 received 4 results and 1 failures\n",
      "DEBUG flwr 2024-03-13 12:54:08,030 | server.py:173 | evaluate_round 4: strategy sampled 2 clients (out of 5)\n",
      "DEBUG flwr 2024-03-13 12:54:08,590 | server.py:187 | evaluate_round 4 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:54:08,591 | server.py:222 | fit_round 5: strategy sampled 5 clients (out of 5)\n",
      "ERROR flwr 2024-03-13 12:54:09,599 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: d28722c665b957007315566401000000, name=DefaultActor.__init__, pid=16466, memory used=0.82GB) was running was 7.33GB / 7.68GB (0.953733), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb6c1abc8aa747fa222c22003e6a843887ccf15d63867524a204d86a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-eb6c1abc8aa747fa222c22003e6a843887ccf15d63867524a204d86a*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.89\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.82\tray::DefaultActor.run\n",
      "16470\t0.70\tray::DefaultActor\n",
      "16471\t0.68\tray::DefaultActor\n",
      "16469\t0.55\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "16461\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 12:54:09,602 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: d28722c665b957007315566401000000, name=DefaultActor.__init__, pid=16466, memory used=0.82GB) was running was 7.33GB / 7.68GB (0.953733), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb6c1abc8aa747fa222c22003e6a843887ccf15d63867524a204d86a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-eb6c1abc8aa747fa222c22003e6a843887ccf15d63867524a204d86a*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.89\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.82\tray::DefaultActor.run\n",
      "16470\t0.70\tray::DefaultActor\n",
      "16471\t0.68\tray::DefaultActor\n",
      "16469\t0.55\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "16461\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-13 12:54:09,604 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 12:54:09,607 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050) where the task (actor ID: 830135e1db0aae0af97069df01000000, name=DefaultActor.__init__, pid=16467, memory used=0.56GB) was running was 7.44GB / 7.68GB (0.968128), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-d6673c3eaf31898055b7adb444bf4551ee00ecadb512b8e11c91c20d*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t0.86\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "16466\t0.68\tray::DefaultActor.run\n",
      "16471\t0.57\tray::DefaultActor\n",
      "16469\t0.57\tray::DefaultActor\n",
      "16467\t0.56\tray::DefaultActor\n",
      "16470\t0.56\tray::DefaultActor\n",
      "16465\t0.23\tray::DefaultActor\n",
      "16457\t0.23\tray::DefaultActor\n",
      "16462\t0.23\tray::DefaultActor\n",
      "16456\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16470)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb6a7f0f0a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m 2024-03-13 12:53:55.531811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=16458)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16467)\u001b[0m 2024-03-13 12:53:58.841970: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16467)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16467)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "DEBUG flwr 2024-03-13 12:54:10,108 | server.py:236 | fit_round 5 received 3 results and 2 failures\n",
      "DEBUG flwr 2024-03-13 12:54:10,112 | server.py:173 | evaluate_round 5: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=16471)\u001b[0m [Client 4] fit, config: {}\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3035 - accuracy: 0.1458\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16466)\u001b[0m [Client 0] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16466)\u001b[0m [Client 0] loss:2.259833335876465, Client 0 accuracy:0.14000000059604645\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:54:10,742 | server.py:187 | evaluate_round 5 received 2 results and 0 failures\n",
      "INFO flwr 2024-03-13 12:54:10,744 | server.py:153 | FL finished in 19.627305863000174\n",
      "INFO flwr 2024-03-13 12:54:10,745 | app.py:226 | app_fit: losses_distributed [(1, 2.3230977058410645), (2, 2.298365831375122), (3, 2.277404546737671), (4, 2.265655040740967), (5, 2.276095747947693)]\n",
      "INFO flwr 2024-03-13 12:54:10,747 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-03-13 12:54:10,749 | app.py:228 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-03-13 12:54:10,752 | app.py:229 | app_fit: losses_centralized []\n",
      "INFO flwr 2024-03-13 12:54:10,754 | app.py:230 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.3230977058410645\n",
       "\tround 2: 2.298365831375122\n",
       "\tround 3: 2.277404546737671\n",
       "\tround 4: 2.265655040740967\n",
       "\tround 5: 2.276095747947693"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-03-13 12:54:49,880 E 16141 16141] (raylet) node_manager.cc:3084: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c486d8a40dd52b9638d49e8b7037bc43403b2fc8a41758ba83a2f050, IP: 172.27.123.152) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.27.123.152`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "model = generate_ann()\n",
    "params = get_parameters(model)# The federated model initial parameters\n",
    "del model\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "        min_fit_clients=NUM_CLIENTS,  # Never sample less than NUM_CLIENTS clients for training\n",
    "        min_evaluate_clients=NUM_CLIENTS//2,  # Never sample less than NUM_CLIENTS//2 clients for evaluation\n",
    "        min_available_clients=NUM_CLIENTS,  # Wait until all NUM_CLIENTS clients are available\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "\n",
    ")\n",
    "\n",
    "# Start simulation. It allows to test on a single device. This code can be executed from the Notebook\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn, #function to create client instances\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,#strategy to integrate the model results\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8e116",
   "metadata": {},
   "source": [
    "This code corresponds to the script running on the server, and it uses the simulation function to test this approach on a single device with the previously mentioned optimization to avoid overloading the device. The code generates NUM_CLIENTS clients and randomly selects all of them (`fraction_fit = 1.0`) to train the model on all of them. After receiving the updates from the clients, the server performs the aggregation strategy before returning the global model to the clients for the next 5 rounds.\n",
    "\n",
    "**Note**: *Be aware that the simulation is a really resource consuming task. You can get some errors and warnings linked to that. There are different [configurations](https://flower.ai/docs/framework/how-to-run-simulations.html) that you can apply to configure the available resources for simulating the process.*\n",
    "\n",
    "One point to highlight is that the framework is not only going to manage the `losses_distributed`, but none of the other metrics. Due to the diverse treatment of those measures, the framework cannot accurately handle the aggregation of these metrics. Users need to tell the framework how to handle and aggregate these custom metrics.\n",
    "\n",
    "The strategy will then call these functions whenever it receives fit or evaluates metrics from clients. The two possible functions are `fit_metrics_aggregation_fn` and `evaluate_metrics_aggregation_fn`. For example, the following code creates the weighted average, and the previous example can be adapted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f79e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-13 12:50:35,288 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2024-03-13 12:50:39,348\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-03-13 12:50:40,751 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 12.0, 'node:__internal_head__': 1.0, 'object_store_memory': 2151206092.0, 'node:172.27.123.152': 1.0, 'memory': 4302412187.0}\n",
      "INFO flwr 2024-03-13 12:50:40,752 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-03-13 12:50:40,753 | app.py:227 | No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2024-03-13 12:50:40,753 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-03-13 12:50:40,769 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 12 actors\n",
      "INFO flwr 2024-03-13 12:50:40,770 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-03-13 12:50:40,771 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2024-03-13 12:50:40,772 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2024-03-13 12:50:40,773 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-03-13 12:50:40,775 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m 2024-03-13 12:50:43.429193: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m 2024-03-13 12:50:43.429222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m 2024-03-13 12:50:43.433381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m 2024-03-13 12:50:43.444263: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m 2024-03-13 12:50:45.681861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "\u001b[2m\u001b[36m(pid=13077)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m 2024-03-13 12:50:48.852976: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m 2024-03-13 12:50:43.429509: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m 2024-03-13 12:50:43.429534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m 2024-03-13 12:50:43.432723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m 2024-03-13 12:50:43.445035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m [Client 0] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:49,494 | server.py:236 | fit_round 1 received 2 results and 0 failures\n",
      "WARNING flwr 2024-03-13 12:50:49,499 | fedavg.py:250 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-03-13 12:50:49,499 | server.py:173 | evaluate_round 1: strategy sampled 1 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 2.2682 - accuracy: 0.1250\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2.6939 - accuracy: 0.0938\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 2.5797 - accuracy: 0.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:50,046 | server.py:187 | evaluate_round 1 received 1 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:50,047 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m [Client 1] evaluate, config: {}\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.3296 - accuracy: 0.1360\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13083)\u001b[0m [Client 1] loss:2.3295726776123047, Client 1 accuracy:0.13600000739097595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:51,103 | server.py:236 | fit_round 2 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:51,106 | server.py:173 | evaluate_round 2: strategy sampled 1 clients (out of 2)\n",
      "DEBUG flwr 2024-03-13 12:50:51,642 | server.py:187 | evaluate_round 2 received 1 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:51,642 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 2s - loss: 2.4165 - accuracy: 0.1562\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3181 - accuracy: 0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:52,670 | server.py:236 | fit_round 3 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:52,674 | server.py:173 | evaluate_round 3: strategy sampled 1 clients (out of 2)\n",
      "DEBUG flwr 2024-03-13 12:50:53,269 | server.py:187 | evaluate_round 3 received 1 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:53,270 | server.py:222 | fit_round 4: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m [Client 1] fit, config: {}\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:54,360 | server.py:236 | fit_round 4 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:54,364 | server.py:173 | evaluate_round 4: strategy sampled 1 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 2.4229 - accuracy: 0.1250\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3983 - accuracy: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 12:50:54,877 | server.py:187 | evaluate_round 4 received 1 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:54,878 | server.py:222 | fit_round 5: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 2.3268 - accuracy: 0.1562\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m [Client 0] evaluate, config: {}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3369 - accuracy: 0.1080\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m [Client 0] loss:2.3369133472442627, Client 0 accuracy:0.1080000028014183\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fc30410ac20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m 2024-03-13 12:50:45.678732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13072)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m 2024-03-13 12:50:48.852535: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=13084)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n",
      "DEBUG flwr 2024-03-13 12:50:56,421 | server.py:236 | fit_round 5 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 12:50:56,424 | server.py:173 | evaluate_round 5: strategy sampled 1 clients (out of 2)\n",
      "DEBUG flwr 2024-03-13 12:50:57,613 | server.py:187 | evaluate_round 5 received 1 results and 0 failures\n",
      "INFO flwr 2024-03-13 12:50:57,615 | server.py:153 | FL finished in 16.839483379000058\n",
      "INFO flwr 2024-03-13 12:50:57,615 | app.py:226 | app_fit: losses_distributed [(1, 2.3295726776123047), (2, 2.3180716037750244), (3, 2.411862373352051), (4, 2.3369133472442627), (5, 2.286043643951416)]\n",
      "INFO flwr 2024-03-13 12:50:57,616 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-03-13 12:50:57,617 | app.py:228 | app_fit: metrics_distributed {'accuracy': [(1, 0.13600000739097595), (2, 0.12800000607967377), (3, 0.09399999678134918), (4, 0.1080000028014183), (5, 0.13600000739097595)]}\n",
      "INFO flwr 2024-03-13 12:50:57,618 | app.py:229 | app_fit: losses_centralized []\n",
      "INFO flwr 2024-03-13 12:50:57,618 | app.py:230 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 2s - loss: 2.3244 - accuracy: 0.0938\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4119 - accuracy: 0.0940\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2860 - accuracy: 0.1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.3295726776123047\n",
       "\tround 2: 2.3180716037750244\n",
       "\tround 3: 2.411862373352051\n",
       "\tround 4: 2.3369133472442627\n",
       "\tround 5: 2.286043643951416\n",
       "History (metrics, distributed, evaluate):\n",
       "{'accuracy': [(1, 0.13600000739097595), (2, 0.12800000607967377), (3, 0.09399999678134918), (4, 0.1080000028014183), (5, 0.13600000739097595)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-03-13 12:51:39,338 E 12753 12753] (raylet) node_manager.cc:3084: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 510f31e97b545463c21e9b5d7b30e5b56d85935023ca4ee791afe656, IP: 172.27.123.152) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.27.123.152`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "from flwr.common import Metrics\n",
    "\n",
    "model = generate_ann()\n",
    "params = get_parameters(model)# The federated model initial parameters\n",
    "del model\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "        min_fit_clients=NUM_CLIENTS,  # Never sample less than NUM_CLIENTS clients for training\n",
    "        min_evaluate_clients=NUM_CLIENTS//2,  # Never sample less than NUM_CLIENTS//2 clients for evaluation\n",
    "        min_available_clients=NUM_CLIENTS,  # Wait until all NUM_CLIENTS clients are available\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ba45",
   "metadata": {},
   "source": [
    "We will revisit the definition of custom strategies in the following unit to define our own strategy and attempt to minimize some of the challenges that federated learning must address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ac8c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now is your turn, why not you try to run your own architecture with this approach. Beaware of the high requirements when we are in a simulated environment.\n",
    "\n",
    "**Note**: The objective of this exercise is to bring together the various topics covered by this notebook. This cell should contain all the necessary code for autonomous execution using Flower simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4ce7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-13 15:43:22,629 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2024-03-13 15:43:27,067\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-03-13 15:43:28,575 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 12.0, 'node:__internal_head__': 1.0, 'object_store_memory': 1818730905.0, 'node:172.27.123.152': 1.0, 'memory': 3637461812.0}\n",
      "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 12.0, 'node:__internal_head__': 1.0, 'object_store_memory': 1818730905.0, 'node:172.27.123.152': 1.0, 'memory': 3637461812.0}\n",
      "INFO flwr 2024-03-13 15:43:28,577 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-03-13 15:43:28,578 | app.py:227 | No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO:flwr:No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2024-03-13 15:43:28,580 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-03-13 15:43:28,601 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 12 actors\n",
      "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 12 actors\n",
      "INFO flwr 2024-03-13 15:43:28,605 | server.py:89 | Initializing global parameters\n",
      "INFO:flwr:Initializing global parameters\n",
      "INFO flwr 2024-03-13 15:43:28,607 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO:flwr:Using initial parameters provided by strategy\n",
      "INFO flwr 2024-03-13 15:43:28,609 | server.py:91 | Evaluating initial parameters\n",
      "INFO:flwr:Evaluating initial parameters\n",
      "INFO flwr 2024-03-13 15:43:28,612 | server.py:104 | FL starting\n",
      "INFO:flwr:FL starting\n",
      "DEBUG flwr 2024-03-13 15:43:28,620 | server.py:222 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "DEBUG:flwr:fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(pid=65399)\u001b[0m 2024-03-13 15:43:31.322187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=65399)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=65399)\u001b[0m 2024-03-13 15:43:33.586481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(pid=65399)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "\u001b[2m\u001b[36m(pid=65399)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m   super().__init__(\n",
      "\u001b[2m\u001b[36m(pid=65397)\u001b[0m 2024-03-13 15:43:31.322175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=65397)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m 2024-03-13 15:43:36.569700: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.1562 - loss: 2.2939\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0938 - loss: 2.3070\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1185 - loss: 2.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:38,208 | server.py:236 | fit_round 1 received 5 results and 0 failures\n",
      "DEBUG:flwr:fit_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2024-03-13 15:43:38,215 | fedavg.py:250 | No fit_metrics_aggregation_fn provided\n",
      "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-03-13 15:43:38,216 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 5)\n",
      "DEBUG:flwr:evaluate_round 1: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0885 - loss: 2.3049\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0729 - loss: 2.3110\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m [Client 1] loss:2.2942466735839844, Client 1 accuracy:0.13500000536441803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:39,270 | server.py:187 | evaluate_round 1 received 2 results and 0 failures\n",
      "DEBUG:flwr:evaluate_round 1 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 15:43:39,272 | server.py:222 | fit_round 2: strategy sampled 5 clients (out of 5)\n",
      "DEBUG:flwr:fit_round 2: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65400)\u001b[0m [Client 2] fit, config: {}\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.0625 - loss: 2.2998\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0755 - loss: 2.3003\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.0677 - loss: 2.3009\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0977 - loss: 2.3115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:44,404 | server.py:236 | fit_round 2 received 5 results and 0 failures\n",
      "DEBUG:flwr:fit_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 15:43:44,428 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 5)\n",
      "DEBUG:flwr:evaluate_round 2: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m [Client 4] loss:2.2999751567840576, Client 4 accuracy:0.07000000029802322\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65403)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:45,214 | server.py:187 | evaluate_round 2 received 2 results and 0 failures\n",
      "DEBUG:flwr:evaluate_round 2 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 15:43:45,216 | server.py:222 | fit_round 3: strategy sampled 5 clients (out of 5)\n",
      "DEBUG:flwr:fit_round 3: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0546 - loss: 2.3163      \n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65403)\u001b[0m [Client 2] loss:2.3165924549102783, Client 2 accuracy:0.07000000029802322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-13 15:43:46,328 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR:flwr:Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 15:43:46,332 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR:flwr:Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65399)\u001b[0m [Client 0] fit, config: {}\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:47,524 | server.py:236 | fit_round 3 received 4 results and 1 failures\n",
      "DEBUG:flwr:fit_round 3 received 4 results and 1 failures\n",
      "DEBUG flwr 2024-03-13 15:43:47,532 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 5)\n",
      "DEBUG:flwr:evaluate_round 3: strategy sampled 2 clients (out of 5)\n",
      "DEBUG flwr 2024-03-13 15:43:48,317 | server.py:187 | evaluate_round 3 received 2 results and 0 failures\n",
      "DEBUG:flwr:evaluate_round 3 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 15:43:48,318 | server.py:222 | fit_round 4: strategy sampled 5 clients (out of 5)\n",
      "DEBUG:flwr:fit_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0933 - loss: 2.2926  \n",
      "\u001b[1m1/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - accuracy: 0.0625 - loss: 2.2881\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1597 - loss: 2.2731  \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0807 - loss: 2.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-13 15:43:49,948 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR:flwr:Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 15:43:49,952 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR:flwr:Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-13 15:43:49,955 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR:flwr:Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 15:43:49,958 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR:flwr:Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65400)\u001b[0m [Client 1] evaluate, config: {}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65400)\u001b[0m [Client 1] loss:2.2836053371429443, Client 1 accuracy:0.12999999523162842\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1706 - loss: 2.2809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:51,317 | server.py:236 | fit_round 4 received 3 results and 2 failures\n",
      "DEBUG:flwr:fit_round 4 received 3 results and 2 failures\n",
      "DEBUG flwr 2024-03-13 15:43:51,326 | server.py:173 | evaluate_round 4: strategy sampled 2 clients (out of 5)\n",
      "DEBUG:flwr:evaluate_round 4: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=65401)\u001b[0m [Client 3] fit, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:52,041 | server.py:187 | evaluate_round 4 received 2 results and 0 failures\n",
      "DEBUG:flwr:evaluate_round 4 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-13 15:43:52,043 | server.py:222 | fit_round 5: strategy sampled 5 clients (out of 5)\n",
      "DEBUG:flwr:fit_round 5: strategy sampled 5 clients (out of 5)\n",
      "ERROR flwr 2024-03-13 15:43:53,052 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR:flwr:Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 15:43:53,053 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR:flwr:Traceback (most recent call last):\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abelj/.local/lib/python3.10/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-13 15:43:53,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-13 15:43:53,057 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR:flwr:Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: b3e5a53d683536a31a2abfbe01000000, name=DefaultActor.__init__, pid=65402, memory used=0.54GB) was running was 7.39GB / 7.68GB (0.961578), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-5694bff12883f97c884e25975907d2c85a3dcc6287e495059baac68e*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.30\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.70\tray::DefaultActor.run\n",
      "65399\t0.67\tray::DefaultActor\n",
      "65403\t0.60\tray::DefaultActor\n",
      "65402\t0.54\tray::DefaultActor\n",
      "65400\t0.53\tray::DefaultActor\n",
      "65398\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR:flwr:Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.27.123.152, ID: 5af08ce514e9e6424f48142f2d7284911f7d0ff10c7ba83efd225bd4) where the task (actor ID: ff6fd52a4f19ad2c4d0f53ba01000000, name=DefaultActor.__init__, pid=65400, memory used=0.75GB) was running was 7.36GB / 7.68GB (0.958666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.123.152`. To see the logs of the worker, use `ray logs worker-6917510529aebbbefb2503eb939904bec9e029fea9229ca21e4cda19*out -ip 172.27.123.152. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "4922\t1.37\t/usr/bin/python3 -m ipykernel_launcher -f /home/abelj/.local/share/jupyter/runtime/kernel-e374753c-b...\n",
      "65401\t0.81\tray::DefaultActor.run\n",
      "65400\t0.75\tray::DefaultActor.run\n",
      "65403\t0.75\tray::DefaultActor.run\n",
      "65399\t0.74\tray::DefaultActor.run\n",
      "65392\t0.22\tray::DefaultActor\n",
      "65396\t0.22\tray::DefaultActor\n",
      "65108\t0.07\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip...\n",
      "65079\t0.06\t/usr/bin/python3 -u /home/abelj/.local/lib/python3.10/site-packages/ray/_private/log_monitor.py --lo...\n",
      "65001\t0.05\t/home/abelj/.local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65403)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f73cfefd000> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=65397)\u001b[0m 2024-03-13 15:43:33.586884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=65397)\u001b[0m /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=65397)\u001b[0m   warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m   super().__init__(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m 2024-03-13 15:43:36.578030: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m /home/abelj/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65402)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2504 - loss: 2.2611  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:53,880 | server.py:236 | fit_round 5 received 3 results and 2 failures\n",
      "DEBUG:flwr:fit_round 5 received 3 results and 2 failures\n",
      "DEBUG flwr 2024-03-13 15:43:53,885 | server.py:173 | evaluate_round 5: strategy sampled 2 clients (out of 5)\n",
      "DEBUG:flwr:evaluate_round 5: strategy sampled 2 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.2188 - loss: 2.2752\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.1849 - loss: 2.2828\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-13 15:43:54,613 | server.py:187 | evaluate_round 5 received 2 results and 0 failures\n",
      "DEBUG:flwr:evaluate_round 5 received 2 results and 0 failures\n",
      "INFO flwr 2024-03-13 15:43:54,615 | server.py:153 | FL finished in 25.995069544000216\n",
      "INFO:flwr:FL finished in 25.995069544000216\n",
      "INFO flwr 2024-03-13 15:43:54,616 | app.py:226 | app_fit: losses_distributed [(1, 2.297110915184021), (2, 2.306170105934143), (3, 2.288422703742981), (4, 2.2725974321365356), (5, 2.280829071998596)]\n",
      "INFO:flwr:app_fit: losses_distributed [(1, 2.297110915184021), (2, 2.306170105934143), (3, 2.288422703742981), (4, 2.2725974321365356), (5, 2.280829071998596)]\n",
      "INFO flwr 2024-03-13 15:43:54,617 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-03-13 15:43:54,619 | app.py:228 | app_fit: metrics_distributed {'accuracy': [(1, 0.10250000283122063), (2, 0.08999999985098839), (3, 0.11249999701976776), (4, 0.22749999910593033), (5, 0.13750000298023224)]}\n",
      "INFO:flwr:app_fit: metrics_distributed {'accuracy': [(1, 0.10250000283122063), (2, 0.08999999985098839), (3, 0.11249999701976776), (4, 0.22749999910593033), (5, 0.13750000298023224)]}\n",
      "INFO flwr 2024-03-13 15:43:54,620 | app.py:229 | app_fit: losses_centralized []\n",
      "INFO:flwr:app_fit: losses_centralized []\n",
      "INFO flwr 2024-03-13 15:43:54,621 | app.py:230 | app_fit: metrics_centralized {}\n",
      "INFO:flwr:app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.297110915184021\n",
       "\tround 2: 2.306170105934143\n",
       "\tround 3: 2.288422703742981\n",
       "\tround 4: 2.2725974321365356\n",
       "\tround 5: 2.280829071998596\n",
       "History (metrics, distributed, evaluate):\n",
       "{'accuracy': [(1, 0.10250000283122063), (2, 0.08999999985098839), (3, 0.11249999701976776), (4, 0.22749999910593033), (5, 0.13750000298023224)]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1063 - loss: 2.2800  \n",
      "\u001b[2m\u001b[36m(DefaultActor pid=65401)\u001b[0m [Client 4] loss:2.2779881954193115, Client 4 accuracy:0.125\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Code to load the dataset\n",
    "import numpy as np\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10_000], y_train[:10_000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into num_clients partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)\n",
    "\n",
    "#TODO Client, client_fn and simulation\n",
    "# Utility functions for the most common operations\n",
    "def get_parameters(net) -> List[np.array]:\n",
    "    return net.get_weights()\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    net.set_weights(parameters)\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    net.fit(trainloader[0], trainloader[1],\n",
    "            epochs=epochs, batch_size=32, steps_per_epoch=3)\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    loss, accuracy = net.evaluate(testloader[0], testloader[1])\n",
    "    return loss, accuracy\n",
    "\n",
    "# Class to contain a Client\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        self.net = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"[Client {self.cid}] loss:{loss}, Client {self.cid} accuracy:{accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    # Create the model\n",
    "    net = generate_ann()\n",
    "    #Take the appropiate part of the dataset\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    #Create and return the Client\n",
    "    return FlowerClient(cid, net, trainloader, valloader)\n",
    "\n",
    "#Simulation code\n",
    "model = generate_ann()\n",
    "params = get_parameters(model)# The federated model initial parameters\n",
    "del model\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "        min_fit_clients=NUM_CLIENTS,  # Never sample less than NUM_CLIENTS clients for training\n",
    "        min_evaluate_clients=NUM_CLIENTS//2,  # Never sample less than NUM_CLIENTS//2 clients for evaluation\n",
    "        min_available_clients=NUM_CLIENTS,  # Wait until all NUM_CLIENTS clients are available\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b411f",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "To conclude this lesson, let's take a closer look at the key point of these strategies, which is the aggregation algorithm. These algorithms are responsible for combining the updates from the clients to generate the global model, and they are defined in the strategies as we have seen. Generally speaking, there are several types of aggregation that can be used in federated learning (Reddi et. al, 2020).  \n",
    "\n",
    "Here are the different types of aggregation that can be used in federated learning:\n",
    "\n",
    "* Federated averaging (`flwr.server.strategy.FedAvg`): In this approach, each device computes an update to the model parameters based on its local data, and these updates are then averaged together to create the global model. This approach is simple and effective, but it can be sensitive to the size of the updates and the quality of the data on each device.\n",
    "\n",
    "* Federated weighted averaging: This approach is similar to federated averaging, but each device's update is given a different weight based on the size of its data set or the quality of its data. This can help to give more influence to devices with larger or higher-quality data.\n",
    "\n",
    "* Federated averaging with momentum (`flwr.server.strategy.FedAvgM`): This approach is similar to federated averaging, but it incorporates a momentum term in order to smooth out the updates and help the model converge more quickly.\n",
    "\n",
    "* Federated stochastic gradient descent(`flwr.server.strategy.FedAdagrad`): In this approach, each device computes an update to the model parameters based on a small batch of its local data, rather than the entire data set. This can help to reduce the communication overhead and improve the convergence rate of the model.\n",
    "\n",
    "* Federated ADAM (`flwr.server.strategy.FedAdam`): This approach is a variant of federated stochastic gradient descent that uses the ADAM optimization algorithm to adaptively adjust the learning rate based on the gradient and second moment estimates.\n",
    "\n",
    "\n",
    "\n",
    "All of the previously mentioned aggregation methods, except for Federated Weighted Averaging, are implemented in the `flwr` framework and can be used with the different strategies. Additionally, there are other less common aggregation methods that can be employed. The choice of aggregation method will ultimately depend on the specific characteristics of the data and the requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1fc5f",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Hard, A., Konen, J., McMahan, H. B., Richemond-Barakat, C., Sivek, J. S., & Talwar, K. (2018). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1812.02903.\n",
    "* Li, Y., Bonawitz, K., & Talwar, K. (2020). Fedprox: An optimizer for communication-efficient federated learning. arXiv preprint arXiv:2002.04283.\n",
    "* McMahan, H. B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2016). Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629.\n",
    "* Yoon, J., Hard, A., Konen, J., McMahan, H. B., & Sohl-Dickstein, J. (2018). Federal regression: A simple and scalable method for heterogeneous federated learning. arXiv preprint arXiv:1812.03862.\n",
    "* Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konen, J., Kumar, S. and McMahan, H.B., 2020. Adaptive federated optimization. arXiv preprint arXiv:2003.00295."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
