{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb1da77",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "Federated learning is a machine learning paradigm that enables decentralized training of a shared model by multiple clients while preserving data privacy. The main idea behind this new paradigm is that each client trains a local model on its own data and then sends only the model updates to a central server, rather than sending the raw data. This allows the model to be trained on a large amount of data without compromising data privacy.\n",
    "\n",
    "Federated learning was first proposed by Google in 2016 (McMahan et al., 2016) and has since been applied in various fields, such as healthcare (Hard et al., 2018), finance (Yoon et al., 2018), and natural language processing (Li et al., 2020).  For example, federated learning could be used to train a model that can make personalized recommendations for each user without requiring the raw data from each user to be shared with a central server. This is the mechanism by which the model is trained on data, while adhering to data privacy requirements.\n",
    "\n",
    "Federated learning is a machine learning approach that offers numerous advantages over traditional centralized methods. Firstly, by leveraging distributed data stores, federated learning can scale to handle significantly larger datasets. Secondly, it prioritizes data privacy by avoiding the transmission of raw data to a central server. Finally, federated learning enables collaboration among multiple clients, allowing them to jointly train a shared model without compromising the security of their individual data. Overall, these benefits make federated learning a promising approach for machine learning in fields where data privacy is of the utmost importance.\n",
    "\n",
    "Federated learning is a process in which a central server distributes a machine learning model to multiple devices. Each device trains the model on its local data and sends the updated model back to the central server. The central server then aggregates the updates from each device to improve the global model. This process is repeated until the model converges and can generate accurate predictions on new data. The key concepts within this process are:\n",
    "\n",
    "\n",
    "* Client: refers to a device or edge node that holds a local dataset and actively participates in the training of the federated model.\n",
    "* Server: represents the central entity that coordinates the training of the federated model and receives model updates from the clients to aggregate into a new version of the global model.\n",
    "* Federated dataset: the collection of decentralized datasets from different clients that are used to train the federated model through collaborative learning.\n",
    "* Federated model: a machine learning model that is trained on the federated dataset using federated learning to make accurate predictions on new data while preserving the privacy of each client's data.\n",
    "* Federated optimization: refers to the process of training the federated model using the decentralized data and model updates from the clients, which enables the model to generalize better on unseen data while preserving the privacy of the clients.\n",
    "* Aggregation: the process of combining the model updates received from the clients into a new version of the global model. This can be done using various methods such as weighted averaging or other approaches.\n",
    "* Rounds: refer to the number of times a federated model is distributed among clients after performing an aggregation to train the model further. The process is repeated until the model converges and achieves a satisfactory level of accuracy.\n",
    "\n",
    "\n",
    "Federated learning is a relatively new approach, and as such, there are few libraries available that have adapted to it. The main actors in this space are TensorFlow Federated, PySyft, OpenMined, and Flower. Of these, TensorFlow Federated is a notable mention, although it is currently only a theoretical approach, as it does not allow for the deployment of the solution and only simulates the federated space. In contrast, Flower allows for the distribution of federated learning, although the necessary modifications can be somewhat challenging. For this tutorial, we have chosen Flower due to its more user-friendly approach and potential for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf254d5",
   "metadata": {},
   "source": [
    "# Introduction to Flower (FLWR)\n",
    "\n",
    "Flower is a Python library that offers tools for implementing the communication and coordination aspects of federated learning. Its design emphasizes ease of use and scalability. It's important to note that Flower is not a learning framework in itself, and as such, it wraps other machine learning frameworks like TensorFlow, PyTorch, or Scikit-learn in the communication layer to enable federated learning.\n",
    "\n",
    "To use Flower for federated learning, you will need to install the library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import flwr as fl\n",
    "except ImportError as err:\n",
    "    !pip install flwr[simulation]\n",
    "    \n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as err:\n",
    "    !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ce39",
   "metadata": {},
   "source": [
    "When setting up a simulation environment, it's best to use the *simulation* keyword with the command to ensure the appropriate environment is loaded. On the other hand, if you plan to use Flower in a distributed setup, the command should be `!pip install flwr` on both the server and client devices. After installing the `flwr` package, you can import it into your Python code using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a157b7b",
   "metadata": {},
   "source": [
    "FLWR provides a range of classes and functions that you can use to set up a federated learning environment, train and evaluate a model, and implement regular updates to the model. You can refer to the FLWR [documentation](https://flower.dev/docs/quickstart-tensorflow.html).  for more information. Before proceeding, it's important to note that the model you define must be serializable so that it can be sent through the network. Not all models are suitable for federated learning. For this example, we'll be using an Artificial Neural Network (ANN) based on TensorFlow, specifically Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model using TensorFlow\n",
    "def generate_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167daad",
   "metadata": {},
   "source": [
    "As we will be using a Deep Learning model defined in TensorFlow, it's recommended to load the data into a `Dataset` class to enable the framework to leverage any available hardware acceleration (such as a GPU on the nodes). The following lines of code load the MNIST dataset and generate a dataset with batches of 32, which should be manageable on most modern machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and partition the dataset that are present on each device\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(10000).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c70c",
   "metadata": {},
   "source": [
    "Now, let's introduce the two pieces of the puzzle: the `Client` and the `Server`. Flower starts a `Server` to coordinate the client devices and perform the orchestration of the model. The server interacts with clients through an interface called `Client`. When the server selects a particular client for training, it sends training instructions over the network. The client receives those instructions and calls one of the Client methods to run your code, which in this case involves training the neural network that we defined earlier.\n",
    "\n",
    "Flower provides a convenient class called NumPyClient, which simplifies the implementation of the Client interface when your workload uses Keras. The NumPyClient interface defines three methods that can be implemented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class to contain the details of the client and be the interface\n",
    "class MyClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, train_dataset, test_dataset):\n",
    "        self.model = net\n",
    "        self.trainloader = train_dataset\n",
    "        self.valloader = test_dataset\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.trainloader, epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "        return self.model.get_weights(), len(x_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.valloader)\n",
    "        return loss, len(x_test), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324ea6a",
   "metadata": {},
   "source": [
    "In the preceding code, we defined the required functions for the client in this particular case. With these functions in place, we can now start a client using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbcd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the client\n",
    "fl.client.start_numpy_client(server_address=\"[::]:8080\", client=MyClient())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d96b24",
   "metadata": {},
   "source": [
    "The string` [::]:8080` specifies the server to which the client should connect. In this case, as the code is being run on the same machine as the server, this address is sufficient. In a truly federated workload, the only thing that needs to be changed is the `server_address` to point the client to the correct server.\n",
    "\n",
    "The other essential piece of the puzzle is the class that will contain the server. This will be in a separate file, for example server.py, and its contents should look something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "\n",
    "fl.server.start_server(config=fl.server.ServerConfig(num_rounds=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684abf1",
   "metadata": {},
   "source": [
    "In this particular case, we can run two clients and a server in separate terminals of the machine. Running two client instances is as simple as executing the `python client.py` command twice in separate terminals, while the server can be started with the `python server.py` command.\n",
    "\n",
    "Upon starting the server, we should receive an output similar to:\n",
    "\n",
    "\n",
    "```shell\n",
    "INFO flower 2022-11-28 11:15:46,741 | app.py:76 | Flower server running (insecure, 3 rounds)\n",
    "INFO flower 2022-11-28 11:15:46,742 | server.py:72 | Getting initial parameters\n",
    "INFO flower 2022-11-28 11:16:01,770 | server.py:74 | Evaluating initial parameters\n",
    "INFO flower 2022-11-28 11:16:01,770 | server.py:87 | [TIME] FL starting\n",
    "DEBUG flower 2022-11-28 11:16:12,341 | server.py:165 | fit_round: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flower 2022-11-28 11:21:17,235 | server.py:177 | fit_round received 2 results and 0 failures\n",
    "DEBUG flower 2022-11-28 11:21:17,512 | server.py:139 | evaluate: strategy sampled 2 clients\n",
    "DEBUG flower 2022-11-28 11:21:29,628 | server.py:149 | evaluate received 2 results and 0 failures\n",
    "DEBUG flower 2022-11-28 11:21:29,696 | server.py:165 | fit_round: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flower 2022-11-28 11:25:59,917 | server.py:177 | fit_round received 2 results and 0 failures\n",
    "DEBUG flower 2022-11-28 11:26:00,227 | server.py:139 | evaluate: strategy sampled 2 clients\n",
    "DEBUG flower 2022-11-28 11:26:11,457 | server.py:149 | evaluate received 2 results and 0 failures\n",
    "DEBUG flower 2022-11-28 11:26:11,530 | server.py:165 | fit_round: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flower 2022-11-28 11:30:43,389 | server.py:177 | fit_round received 2 results and 0 failures\n",
    "DEBUG flower 2022-11-28 11:30:43,630 | server.py:139 | evaluate: strategy sampled 2 clients\n",
    "DEBUG flower 2022-11-28 11:30:53,384 | server.py:149 | evaluate received 2 results and 0 failures\n",
    "INFO flower 2022-11-28 11:30:53,384 | server.py:122 | [TIME] FL finished in 891.6143046000007\n",
    "INFO flower 2022-11-28 11:30:53,385 | app.py:109 | app_fit: losses_distributed [(1, 2.3196680545806885), (2, 2.3202896118164062), (3, 2.1818180084228516)]\n",
    "INFO flower 2022-11-28 11:30:53,385 | app.py:110 | app_fit: accuracies_distributed []\n",
    "INFO flower 2022-11-28 11:30:53,385 | app.py:111 | app_fit: losses_centralized []\n",
    "INFO flower 2022-11-28 11:30:53,385 | app.py:112 | app_fit: accuracies_centralized []\n",
    "DEBUG flower 2022-11-28 11:30:53,442 | server.py:139 | evaluate: strategy sampled 2 clients\n",
    "DEBUG flower 2022-11-28 11:31:02,848 | server.py:149 | evaluate received 2 results and 0 failures\n",
    "INFO flower 2022-11-28 11:31:02,848 | app.py:121 | app_evaluate: federated loss: 2.1818180084228516\n",
    "INFO flower 2022-11-28 11:31:02,848 | app.py:125 | app_evaluate: results [('ipv4:127.0.0.1:31539', EvaluateRes(loss=2.1818180084228516, num_examples=10000, accuracy=0.0, metrics={'accuracy': 0.21610000729560852})), ('ipv4:127.0.0.1:31540', EvaluateRes(loss=2.1818180084228516, num_examples=10000, accuracy=0.0, metrics={'accuracy': 0.21610000729560852}))]\n",
    "INFO flower 2022-11-28 11:31:02,848 | app.py:127 | app_evaluate: failures [] flower 2020-11-18 11:07:56,396 | app.py:77 | app_evaluate: failures []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8f5a6",
   "metadata": {},
   "source": [
    "With that, the first federated learning approach is completed. As you can see, the system goes through three rounds of fitting and evaluating on all clients before the results are retrieved, aggregated, and redistributed to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00137c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Implement the client and server code in two separate files and compare the results with those presented here. Were your results similar?\n",
    "\n",
    "`Answer here`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25335",
   "metadata": {},
   "source": [
    "# Updating parameters\n",
    "The key element in this kind of approach is that the server sends the global model parameters to the client, and the client updates the local model with the parameters received from the server. It then trains the model on the local data, which changes the model parameters locally. After training, the updated model parameters are sent back to the server, or alternatively, only the gradients are sent back to the server, not the full model parameters.\n",
    "\n",
    "\n",
    "In `flwr`, this communication is essentially done by two helper functions for loading and retrieving local parameters: `set_parameters` and `get_parameters`. This requirement fits well with non-state approaches such as **PyTorch** or **JAX**. As demonstrated in the previous example, `flwr` can also be used with **TensorFlow** or even **scikit-learn**.\n",
    "\n",
    "As a result, the basic structure for any client using this library has the same format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467be0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN is only an example\n",
    "#This is an example in Pytrorch and you will use this code to adapt it to tensorflow a little lower\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader # Dataset for train\n",
    "        self.valloader = valloader # Dataset to validate\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net) # To be implemented specific for the framework\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters) # also to be implemented specificly for the framework\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e352",
   "metadata": {},
   "source": [
    "In Flower, clients can be created by extending either the `flwr.client.Client` or `flwr.client.NumPyClient` classes. In the previous example, we used `NumPyClient` because it is easier to implement and requires less code as a template. Along with the extended class, there are three main methods that need to be implemented:\n",
    "\n",
    "* `get_parameters`: Returns the current local model parameters.\n",
    "* `fit`: Receives model parameters from the server, trains the model parameters on the local data, and returns the (updated) model parameters to the server.\n",
    "* `evaluate`: Receives model parameters from the server, evaluates the model parameters on the local data, and returns the evaluation result to the server.\n",
    "\n",
    "As you can see, the `MyClient` class implemented in the previous example follows this same structure.\n",
    "\n",
    "\n",
    "#### Be aware: \n",
    "Sometimes, especially when we are simulating multiple clients on a single device, it can be useful to use a function to create the client when it is required. This is particularly important in stateless frameworks, such as PyTorch, which can benefit from a more efficient implementation that creates clients only when they are required for training or evaluation. For example, the following code loads different examples for each client before discarding them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN\n",
    "# Some as above this is the implementation in Pytorch same case that above\n",
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    net = Net().to(DEVICE) # This should be adapted according to the framework\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return FlowerClient(net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb598",
   "metadata": {},
   "source": [
    "Note that `myClient` cannot be used in the same sense because of the state that it keeps internally through the function `generate_ann`. However, if this state is removed, it can be used in the same way.\n",
    "\n",
    "The clients are now set up to load, fit, and evaluate. However, we need to integrate the results from the different clients. In Flower terminology, this is known as a strategy, such as the *Federated Average (FedAvg)* strategy. In a first approach, we can use the built-in implementations of the framework, although custom strategies can also be used. Let's see an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "        min_fit_clients=10,  # Never sample less than 10 clients for training\n",
    "        min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "        min_available_clients=10,  # Wait until all 10 clients are available\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=10,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8e116",
   "metadata": {},
   "source": [
    "This code corresponds to the script running on the server, and it uses the simulation function to test this approach on a single device with the previously mentioned optimization to avoid overloading the device. The code generates 10 clients and randomly selects all of them (`fraction_fit = 1.0`) to train the model on all of them. After receiving the updates from the clients, the server performs the aggregation strategy before returning the global model to the clients for the next 5 rounds.\n",
    "\n",
    "One point to highlight is that the framework is not only going to manage the `losses_distributed`, but none of the other metrics. Due to the diverse treatment of those measures, the framework cannot accurately handle the aggregation of these metrics. Users need to tell the framework how to handle and aggregate these custom metrics.\n",
    "\n",
    "The strategy will then call these functions whenever it receives fit or evaluates metrics from clients. The two possible functions are `fit_metrics_aggregation_fn` and `evaluate_metrics_aggregation_fn`. For example, the following code creates the weighted average, and the previous example can be adapted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=10,\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=10,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ba45",
   "metadata": {},
   "source": [
    "We will revisit the definition of custom strategies in the following unit to define our own strategy and attempt to minimize some of the challenges that federated learning must address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ac8c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "To test our implementation, we need to run the simulation with the CIFAR-10 dataset in a simulated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(n_clients):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    " \n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    #Normalize data\n",
    "    #TODO\n",
    "    \n",
    "    #Prepare the datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    train_partition_size = len(x_train) // n_clients\n",
    "    test_partition_size = len(x_test) // n_clients\n",
    "    \n",
    "    #Randomize the datasets\n",
    "    \n",
    "    train_dataset = train_dataset.shuffle(10_000)\n",
    "    test_dataset = train_dataset.shuffle(2_000)\n",
    "\n",
    "    # Split each partition \n",
    "    train_ds = []\n",
    "    test_ds = []\n",
    "    for _ in range(n_clients):\n",
    "        train_ds.append(train_dataset.take(train_partition_size))    \n",
    "        train_dataset = train_dataset.skip(train_partition_size)\n",
    "        test_ds.append(test_dataset.take(test_partition_size))\n",
    "        test_dataset = test_dataset.skip(test_partition_size)\n",
    "\n",
    "train_ds, test_ds = load_datasets()\n",
    "\n",
    "#TODO Client, client_fn and simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b411f",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "To conclude this lesson, let's take a closer look at the key point of these strategies, which is the aggregation algorithm. These algorithms are responsible for combining the updates from the clients to generate the global model, and they are defined in the strategies as we have seen. Generally speaking, there are several types of aggregation that can be used in federated learning (Reddi et. al, 2020).  \n",
    "\n",
    "Here are the different types of aggregation that can be used in federated learning:\n",
    "\n",
    "* Federated averaging (`flwr.server.strategy.FedAvg`): In this approach, each device computes an update to the model parameters based on its local data, and these updates are then averaged together to create the global model. This approach is simple and effective, but it can be sensitive to the size of the updates and the quality of the data on each device.\n",
    "\n",
    "* Federated weighted averaging: This approach is similar to federated averaging, but each device's update is given a different weight based on the size of its data set or the quality of its data. This can help to give more influence to devices with larger or higher-quality data.\n",
    "\n",
    "* Federated averaging with momentum (`flwr.server.strategy.FedAvgM`): This approach is similar to federated averaging, but it incorporates a momentum term in order to smooth out the updates and help the model converge more quickly.\n",
    "\n",
    "* Federated stochastic gradient descent(`flwr.server.strategy.FedAdagrad`): In this approach, each device computes an update to the model parameters based on a small batch of its local data, rather than the entire data set. This can help to reduce the communication overhead and improve the convergence rate of the model.\n",
    "\n",
    "* Federated ADAM (`flwr.server.strategy.FedAdam`): This approach is a variant of federated stochastic gradient descent that uses the ADAM optimization algorithm to adaptively adjust the learning rate based on the gradient and second moment estimates.\n",
    "\n",
    "\n",
    "\n",
    "All of the previously mentioned aggregation methods, except for Federated Weighted Averaging, are implemented in the `flwr` framework and can be used with the different strategies. Additionally, there are other less common aggregation methods that can be employed. The choice of aggregation method will ultimately depend on the specific characteristics of the data and the requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1fc5f",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Hard, A., Konečný, J., McMahan, H. B., Richemond-Barakat, C., Sivek, J. S., & Talwar, K. (2018). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1812.02903.\n",
    "* Li, Y., Bonawitz, K., & Talwar, K. (2020). Fedprox: An optimizer for communication-efficient federated learning. arXiv preprint arXiv:2002.04283.\n",
    "* McMahan, H. B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2016). Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629.\n",
    "* Yoon, J., Hard, A., Konečný, J., McMahan, H. B., & Sohl-Dickstein, J. (2018). Federal regression: A simple and scalable method for heterogeneous federated learning. arXiv preprint arXiv:1812.03862.\n",
    "* Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konečný, J., Kumar, S. and McMahan, H.B., 2020. Adaptive federated optimization. arXiv preprint arXiv:2003.00295."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f6d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
