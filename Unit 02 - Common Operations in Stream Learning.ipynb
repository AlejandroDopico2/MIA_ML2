{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c68a0c",
   "metadata": {},
   "source": [
    "# Moving from standard to stream learning\n",
    "\n",
    "This tutorial is inspired by the [river library](https://riverml.xyz) examples and recipies. So, a closer look to those ones is encorage to any person who want to indepth on this particular topics.\n",
    "\n",
    "Generaly speaking, nearly any approach in machine learning can be summarize in the following steps:\n",
    "\n",
    "1. Defining the problem\n",
    "    1. Identify the kind of problem (supervised vs un supervised, classification vs regression, etc.)\n",
    "1. Loading the data\n",
    "2. Preprocessing that data\n",
    "    1. Feature extraction\n",
    "    2. Feature selection\n",
    "3. Fitting the model\n",
    "    1. Ajusted the hyperparameters\n",
    "4. Evaluate the model according certain measures\n",
    "\n",
    "Let's see a classical example which ticks all those steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db20f47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ROC AUC: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9776</span> <span style=\"font-weight: bold\">(</span>± <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0165</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ROC AUC: \u001b[1;36m0.9776\u001b[0m \u001b[1m(\u001b[0m± \u001b[1;36m0.0165\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from rich import print\n",
    "\n",
    "\n",
    "# Load the data\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Prepare the pipeline with the preprocessing steps and the model to be used\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),                       # Prepapare the data centered in avg 0 and std 1\n",
    "    ('extractor', PCA(0.95)),                          # Extract features that represents 0.95 of the variability\n",
    "    ('classifier', LogisticRegression(solver='lbfgs')) # Last step the model to perform the classificaction\n",
    "])\n",
    "\n",
    "# Define a determistic cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the performance for each fold of the cross valiada\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(pipe, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c3771",
   "metadata": {},
   "source": [
    "So, what is the problem with this approach, it has no problem but some downsize can be detected. First and foremost, if the data returned by `load_breast_cancer` had been too big for the memory of your computer, the program would have crashed. Although some techniques can be applied to minimize this threat, such as optimizing the typing, or the use of sparse storage for the data there is a limit to the optimizations available. If your dataset potentially can have millions of samples which can weigh hundreds of gigabytes, you will need some special hardware for sure. One solution is to do out-of-core learning; that is, algorithms that can learn by being presented the data in chunks or mini-batches, but potential issues arise from these considering the order and the local minima of the search space.\n",
    "\n",
    "Another potential issue is the incorporation of new data into the model. Traditional approaches require starting from scratch with a new dataset result of the combination of the old data with the new samples available. This is particularly problematic in real-time applications where you have new data available now and then. In many real applications, the solution is to perform a continuous integration pipeline which can deploy a new model nearly several times per minute.\n",
    "\n",
    "Finally, another problem of the traditional approach is the availability of the features. Some features might not be accessible at the particular point in time you are at, for example, a particular state of an object or an amount at a warehouse is not available in a week or month because the data is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe261e",
   "metadata": {},
   "source": [
    "# Incremental learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d754d3f",
   "metadata": {},
   "source": [
    "Incremental learning is also called online learning or stream learning, however, if you google \"online learning\", it is unlikely that the results are what you were looking for. Consequently, the terms \"incremental learning\" and \"stream learning\" are usually preferred. Behind incremental learning is a very clear idea which it is is to fit a model according to a continuous stream of data. In other words, the data isn't available in its entirety, but rather the observations are provided one by one. For example, imagine the previous classical example which, instead of the data set, we have a reference point providing one sample at a time. This can be simulated with a simple loop, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi in zip(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a256cb",
   "metadata": {},
   "source": [
    "Since the data is already in memory, this is not the ideal scenario to exemplify it. However, keep in mind that in this particular case we are using each sample a single time. This can also be used in the same way by iterating on the results from a CSV file, a Kafka stream, an SQL query, etc.\n",
    "\n",
    "One particular point to be aware of is the fact that in this example `xi` is an instance of `numpy.array`. By its design, the library that we are using, `river`, uses the class `dict` as the base of its behaviour. The authors of the library assume a point of view where each observation represents a single sample which could make sense from the point of view of a stream of data. It is no limitation but something to be aware of when performance is required. Remember that `dict` is implemented in **Python**, while the `numpy.array` is implemented at a low level in **C** and **Fortran**. One of the advantages of using `dict` is easy access to the features and a clearer program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9133930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In numpy.array format:<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.760e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.454e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.792e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.810e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.263e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.362e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.587e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.884e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.857e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.915e+01</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.189e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.660e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.676e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.783e-03</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.037e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.916e+01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.686e+02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.996e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.444e-02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000e+00</span>\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.871e-01</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.039e-02</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In numpy.array format:\u001b[1m[\u001b[0m\u001b[1;36m7.760e+00\u001b[0m \u001b[1;36m2.454e+01\u001b[0m \u001b[1;36m4.792e+01\u001b[0m \u001b[1;36m1.810e+02\u001b[0m \u001b[1;36m5.263e-02\u001b[0m \u001b[1;36m4.362e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m1.587e-01\u001b[0m \u001b[1;36m5.884e-02\u001b[0m \u001b[1;36m3.857e-01\u001b[0m \u001b[1;36m1.428e+00\u001b[0m \u001b[1;36m2.548e+00\u001b[0m \u001b[1;36m1.915e+01\u001b[0m\n",
       " \u001b[1;36m7.189e-03\u001b[0m \u001b[1;36m4.660e-03\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m2.676e-02\u001b[0m \u001b[1;36m2.783e-03\u001b[0m \u001b[1;36m9.456e+00\u001b[0m\n",
       " \u001b[1;36m3.037e+01\u001b[0m \u001b[1;36m5.916e+01\u001b[0m \u001b[1;36m2.686e+02\u001b[0m \u001b[1;36m8.996e-02\u001b[0m \u001b[1;36m6.444e-02\u001b[0m \u001b[1;36m0.000e+00\u001b[0m \u001b[1;36m0.000e+00\u001b[0m\n",
       " \u001b[1;36m2.871e-01\u001b[0m \u001b[1;36m7.039e-02\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In dict format: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean radius'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.76</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean texture'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.54</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean perimeter'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.92</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">area'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">181.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean smoothness'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05263</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean compactness'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04362</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean concavity'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'mean concave points'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean symmetry'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1587</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mean fractal dimension'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05884</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'radius error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3857</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'texture error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.428</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'perimeter error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.548</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'area error'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.15</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'smoothness error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007189</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'compactness error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00466</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'concavity error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'concave points error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'symmetry error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02676</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fractal dimension error'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002783</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'worst radius'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.456</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst texture'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.37</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst perimeter'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59.16</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst area'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268.6</span>,\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'worst smoothness'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08996</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst compactness'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06444</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst concavity'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">concave points'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst symmetry'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2871</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'worst fractal dimension'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07039</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In dict format: \u001b[1m{\u001b[0m\u001b[32m'mean radius'\u001b[0m: \u001b[1;36m7.76\u001b[0m, \u001b[32m'mean texture'\u001b[0m: \u001b[1;36m24.54\u001b[0m, \u001b[32m'mean perimeter'\u001b[0m: \u001b[1;36m47.92\u001b[0m, \u001b[32m'mean \u001b[0m\n",
       "\u001b[32marea'\u001b[0m: \u001b[1;36m181.0\u001b[0m, \u001b[32m'mean smoothness'\u001b[0m: \u001b[1;36m0.05263\u001b[0m, \u001b[32m'mean compactness'\u001b[0m: \u001b[1;36m0.04362\u001b[0m, \u001b[32m'mean concavity'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "\u001b[32m'mean concave points'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \u001b[32m'mean symmetry'\u001b[0m: \u001b[1;36m0.1587\u001b[0m, \u001b[32m'mean fractal dimension'\u001b[0m: \u001b[1;36m0.05884\u001b[0m, \n",
       "\u001b[32m'radius error'\u001b[0m: \u001b[1;36m0.3857\u001b[0m, \u001b[32m'texture error'\u001b[0m: \u001b[1;36m1.428\u001b[0m, \u001b[32m'perimeter error'\u001b[0m: \u001b[1;36m2.548\u001b[0m, \u001b[32m'area error'\u001b[0m: \n",
       "\u001b[1;36m19.15\u001b[0m, \u001b[32m'smoothness error'\u001b[0m: \u001b[1;36m0.007189\u001b[0m, \u001b[32m'compactness error'\u001b[0m: \u001b[1;36m0.00466\u001b[0m, \u001b[32m'concavity error'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \n",
       "\u001b[32m'concave points error'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \u001b[32m'symmetry error'\u001b[0m: \u001b[1;36m0.02676\u001b[0m, \u001b[32m'fractal dimension error'\u001b[0m: \u001b[1;36m0.002783\u001b[0m, \n",
       "\u001b[32m'worst radius'\u001b[0m: \u001b[1;36m9.456\u001b[0m, \u001b[32m'worst texture'\u001b[0m: \u001b[1;36m30.37\u001b[0m, \u001b[32m'worst perimeter'\u001b[0m: \u001b[1;36m59.16\u001b[0m, \u001b[32m'worst area'\u001b[0m: \u001b[1;36m268.6\u001b[0m,\n",
       "\u001b[32m'worst smoothness'\u001b[0m: \u001b[1;36m0.08996\u001b[0m, \u001b[32m'worst compactness'\u001b[0m: \u001b[1;36m0.06444\u001b[0m, \u001b[32m'worst concavity'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \u001b[32m'worst \u001b[0m\n",
       "\u001b[32mconcave points'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \u001b[32m'worst symmetry'\u001b[0m: \u001b[1;36m0.2871\u001b[0m, \u001b[32m'worst fractal dimension'\u001b[0m: \u001b[1;36m0.07039\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"In numpy.array format:{xi}\\n\")\n",
    "print(f\"In dict format: {dict(zip(dataset.feature_names, xi))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6196f",
   "metadata": {},
   "source": [
    "Although it is not complicated, `river` provides a convinient function to transform the datasets from `scikit-learn`to the required format. For the examples developed in this notebook, you can use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af3f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aedc52",
   "metadata": {},
   "source": [
    "Therefore, the change in our code seems pretty focalized and not quite important. By simply changing our well-known batches by the stream of data a lot of things cannot be done in the same way. For example, something so trivial and commonly done such as the standardisation of the data, i.e. scaling the data to have mean 0 and variance 1. This simple operation in the batched approach changes into something more complicated in the stream data because we don't know the values of the mean and the standard deviation before actually going through all the data. For this problem, a possible approach is to perform the first pass over the data to compute the necessary values and then scale the values during a second pass. However, this approach goes against out objective of only looking at the data once and many times it is not possible. So, how we can solve this, the answer comes in the shape o what is called computing running statistics or moving statistics. By using those we do not use the exact mean and standard deviation but an estimation which can be updated with each new value. For example, denoting the mean as $\\mu_t$ and the count of element at any moment $n_t$, the mean can be easily updated at any moment by applying the following function:\n",
    "$$\n",
    "  n_{t+1} = n_t +1 \\\\    \n",
    "  \\mu_{t+1} = \\mu_t +\\frac{x - \\mu_t}{n_{t+1}}    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e6560",
   "metadata": {},
   "source": [
    "In the same way, for the variance($\\sigma$), the previous formula can be completed with:\n",
    "$$\n",
    "  s_{t+1} = s_t + (x-\\mu_t)\\times(x-\\mu_{t+1})\\\\\n",
    "  \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}}\n",
    "$$\n",
    "\n",
    "where $s_t$  is a running sum of squares and $\\sigma_t$ is the running variance at time $t$. These four formulae can be easily rewritten in Python for example, take the `mean radius` as our Guinea Pig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96b88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.127</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running mean: \u001b[1;36m14.127\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running variance: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.397</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running variance: \u001b[1;36m12.397\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's inicialize the variables before any rolling calculation\n",
    "n, mean, s, variance = 0, 0, 0, 0\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    n += 1\n",
    "    mean_t = mean\n",
    "    mean += (xi['mean radius'] - mean) / n\n",
    "    s += (xi['mean radius'] - mean_t) * (xi['mean radius'] - mean)\n",
    "    variance = s / n\n",
    "\n",
    "print(f'Running mean: {mean:.3f}')\n",
    "print(f'Running variance: {variance:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047a5fd",
   "metadata": {},
   "source": [
    "Now, compare the results with the ones implementations of `numpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4b5123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> mean: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.127</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m mean: \u001b[1;36m14.127\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> variance: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.397</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m variance: \u001b[1;36m12.397\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = list(dataset.feature_names).index('mean radius')\n",
    "print(f'True mean: {np.mean(X[:, i]):.3f}')\n",
    "print(f'True variance: {np.var(X[:, i]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3af056",
   "metadata": {},
   "source": [
    "The results are identical with a key difference, the `numpy`implementation requires all data to be available for the calculation while the another is calculated as the problem progress in its development. Therefore, we should keep in mind that the results with a few instances are not going to be very accurate. \n",
    "\n",
    "SO, once we can calculate different statistics on the data, the next step is to use those in order to normalize or standarize the data in a similar way as the batched approach. In `river`, several functions has been implemented in this sense in the `preprocessing` module. For example, to standarize all the features of the previous example we can perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d2928b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer()):\n",
    "    scaler = scaler.learn_one(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7ab89",
   "metadata": {},
   "source": [
    "So, now that we are scaling the data, we can step forward and perform some machine learning approach. In this example, we are going to implement a linear regression based on an *stochastic gradient descent (SGD)*. The idea behind it is to optimize the output of the linear regression based on the maximum variance of the error made between the prediction and the true output. In this particular case, the *Squared Error* is the loss function in order to elude possible compensations due to signal change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3298b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ROC AUC: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ROC AUC: \u001b[1;36m0.9896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river.linear_model import LogisticRegression\n",
    "from river.optim import SGD\n",
    "\n",
    "scaler = StandardScaler()\n",
    "optimizer = SGD(lr=0.01)\n",
    "log_reg = LogisticRegression(optimizer)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in stream.iter_sklearn_dataset(load_breast_cancer(), shuffle=True, seed=42):\n",
    "\n",
    "    # Scale the features\n",
    "    xi_scaled = scaler.learn_one(xi).transform_one(xi)\n",
    "\n",
    "    # Test the current model on the new \"unobserved\" sample\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Train the model with the new sample\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Store the truth and the prediction\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c805e6",
   "metadata": {},
   "source": [
    "The results seem to be a little better than the ones from `scikit-learn`. Let's make a proper comparison, to do that, we should use the same CV folds ffor each approach and compaare the results. Although, we could define the same process quite easily, in order to make them completely comparable, `river` provides a module called `compat` which improves the compatibility with other Python libreries such as skelearn. So, we can use the function `convert_river_to_sklearn` to obtain an object perfectly compatible with the functions of `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7b732fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ROC AUC: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9639</span> <span style=\"font-weight: bold\">(</span>± <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0158</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ROC AUC: \u001b[1;36m0.9639\u001b[0m \u001b[1m(\u001b[0m± \u001b[1;36m0.0158\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river.compose import Pipeline\n",
    "from river.compat import convert_river_to_sklearn\n",
    "\n",
    "\n",
    "# We define a Pipeline, becasuse we need a single object \n",
    "model = Pipeline(\n",
    "    ('scale', StandardScaler()),\n",
    "    ('ml_model', LogisticRegression())\n",
    ")\n",
    "\n",
    "# This funtion returns an object of type SKLRegressorWrapper \n",
    "# which is compatible with the signature of sklearn\n",
    "model = convert_river_to_sklearn(model)\n",
    "\n",
    "# Now, we can proceed using the cross_val_score from \n",
    "# sklearn with the River wrapped model\n",
    "scores = cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Let's compare the results\n",
    "print(f'ROC AUC: {scores.mean():.4f} (± {scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab62582",
   "metadata": {},
   "source": [
    "Although they are lower than the previous test, the results are comparable to the batch learning approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcecff",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Although it has been barely introduce in the previous section, many times the \"flow\" of information in a machine learning approach can be modeled as a sequence of steps. In order to perform this operation, some libreries such as `scikit=learn` or `pandas` have the definition of objects to contain this type of definition. \n",
    "\n",
    "However, in practice, you are going to see very few use of the pipes because, for most developers, it is easier to think their workflows in a procedural way instead of a declarative one. The main reason is due to the mayority of batch machine learning, however, when wetalk about online learning, pipelines seems the more natural way to reason. Let's compare the same process perform with a procedural approach and with a declarative one which is based on pipelines.\n",
    "\n",
    "Now we are going to change the problem for one of Kaggle Competitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75f1207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Data from the Kaggle Recruit Restaurants challenge.\n",
       "\n",
       "The goal is to predict the number of visitors in each of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">829</span> Japanese restaurants over a \n",
       "priod\n",
       "of roughly <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> weeks. The data is ordered by date and then by restaurant ID.\n",
       "\n",
       "      Name  Restaurants                                                               \n",
       "      Task  Regression                                                                \n",
       "   Samples  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">252</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span>                                                                   \n",
       "  Features  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>                                                                         \n",
       "    Sparse  <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                                                                     \n",
       "      Path  <span style=\"color: #800080; text-decoration-color: #800080\">/home/quique/river_data/Restaurants/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">kaggle_recruit_restaurants.csv</span>        \n",
       "       URL  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip</span>\n",
       "      Size  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.54</span> MB                                                                  \n",
       "Downloaded  <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Data from the Kaggle Recruit Restaurants challenge.\n",
       "\n",
       "The goal is to predict the number of visitors in each of \u001b[1;36m829\u001b[0m Japanese restaurants over a \n",
       "priod\n",
       "of roughly \u001b[1;36m16\u001b[0m weeks. The data is ordered by date and then by restaurant ID.\n",
       "\n",
       "      Name  Restaurants                                                               \n",
       "      Task  Regression                                                                \n",
       "   Samples  \u001b[1;36m252\u001b[0m,\u001b[1;36m108\u001b[0m                                                                   \n",
       "  Features  \u001b[1;36m7\u001b[0m                                                                         \n",
       "    Sparse  \u001b[3;91mFalse\u001b[0m                                                                     \n",
       "      Path  \u001b[35m/home/quique/river_data/Restaurants/\u001b[0m\u001b[95mkaggle_recruit_restaurants.csv\u001b[0m        \n",
       "       URL  \u001b[4;94mhttps://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip\u001b[0m\n",
       "      Size  \u001b[1;36m27.54\u001b[0m MB                                                                  \n",
       "Downloaded  \u001b[3;91mFalse\u001b[0m                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "from river.datasets import Restaurants\n",
    "\n",
    "data = Restaurants()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf71766",
   "metadata": {},
   "source": [
    "Lets check the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72a9e508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'store_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'air_04341b588bde96cd'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'genre_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Izakaya'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'area_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tōkyō-to Nerima-ku Toyotamakita'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.7356234</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139.6516577</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'store_id'\u001b[0m: \u001b[32m'air_04341b588bde96cd'\u001b[0m,\n",
       "        \u001b[32m'date'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2016\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'is_holiday'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'genre_name'\u001b[0m: \u001b[32m'Izakaya'\u001b[0m,\n",
       "        \u001b[32m'area_name'\u001b[0m: \u001b[32m'Tōkyō-to Nerima-ku Toyotamakita'\u001b[0m,\n",
       "        \u001b[32m'latitude'\u001b[0m: \u001b[1;36m35.7356234\u001b[0m,\n",
       "        \u001b[32m'longitude'\u001b[0m: \u001b[1;36m139.6516577\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1;36m10\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b8c26",
   "metadata": {},
   "source": [
    "Lets bwgging with the procedural approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a71f8440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.316538</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MAE: \u001b[1;36m8.316538\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river import feature_extraction, linear_model, metrics, preprocessing, stats, utils\n",
    "\n",
    "# Let's create the average of the last 7, 14 and 21 days\n",
    "features = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21))\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "model = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Derive date features\n",
    "    x['weekday'] = x['date'].weekday()\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "\n",
    "    # Process the rolling means of the target  \n",
    "    for mean in features:\n",
    "        x = {**x, **mean.transform_one(x)}\n",
    "        mean.learn_one(x, y)\n",
    "\n",
    "    # Remove the key/value pairs that aren't features\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "\n",
    "    # Rescale the data\n",
    "    x = scaler.learn_one(x).transform_one(x)\n",
    "\n",
    "    # Fit the linear regression\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c238cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.23103573677646685</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'weekday'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0292832579142892</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is_weekend'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6249280076334165</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'y_mean_by_store_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3980979075298516</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'is_holiday'\u001b[0m: \u001b[1;36m-0.23103573677646685\u001b[0m,\n",
       "    \u001b[32m'weekday'\u001b[0m: \u001b[1;36m1.0292832579142892\u001b[0m,\n",
       "    \u001b[32m'is_weekend'\u001b[0m: \u001b[1;36m1.6249280076334165\u001b[0m,\n",
       "    \u001b[32m'y_mean_by_store_id'\u001b[0m: \u001b[1;36m-1.3980979075298516\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the last sample\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f347fb",
   "metadata": {},
   "source": [
    "Now, lets rewrite the same code but with an approach more declarative based on the steps od the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e257728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MAE: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.41379</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MAE: \u001b[1;36m8.41379\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "# funtion to transform the data in the same features used previously\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "# Build the pipeline with the same steps that has been previously done\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in data:\n",
    "\n",
    "    # Make a prediction without using the target\n",
    "    y_pred = model.predict_one(x)\n",
    "\n",
    "    # Update the model using the target\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1dcb03",
   "metadata": {},
   "source": [
    "As you can see we have arrange all the feature calculation and transform in an object `TransformerUnion`. The idea is to been able to calculate all features in parallel for each element that is passed to this step. Additionaly, the `fot-loop` has become an image of the typical steps inside a online learning system and, therefore, the built=in evaluation function can be used to simplify, evenmore, the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d86d5f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.41379"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b64cec",
   "metadata": {},
   "source": [
    "Although the code is perfectly fine like this, because now we can use the built-in functions of the library. Some additional simplifications can be done. First, and opposite to the pipelines in `scikit-learn`, the name of the steps is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c9738f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.41379"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21))\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67392efd",
   "metadata": {},
   "source": [
    "In this case the library will infer one based on the order of the operations provided.\n",
    "\n",
    "The next simplification comes from the fact that the pipeline can be declare with mathematical operations. Fist, use `+` to declare a `TransformerUnion` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdd6ceca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.41379"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21)),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f88a0",
   "metadata": {},
   "source": [
    "Likewhise we can use the `|` operator to assemble steps into a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8eed5648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.41379"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 7)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 14)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), 21))\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "model = model | compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7746f",
   "metadata": {},
   "source": [
    "One final simply fication comes from the dact that `river`automatically encapsulates functions in the `FuncTrandform` so the final declarative model could be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6042777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,000] MAE: 8.772938\n",
      "[40,000] MAE: 8.906363\n",
      "[60,000] MAE: 8.925668\n",
      "[80,000] MAE: 8.65652\n",
      "[100,000] MAE: 8.517044\n",
      "[120,000] MAE: 8.459466\n",
      "[140,000] MAE: 8.395895\n",
      "[160,000] MAE: 8.392736\n",
      "[180,000] MAE: 8.486566\n",
      "[200,000] MAE: 8.405058\n",
      "[220,000] MAE: 8.360717\n",
      "[240,000] MAE: 8.411629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAE: 8.41379"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n))\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data, model=model, metric=metrics.MAE(), print_every=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40c1dc",
   "metadata": {},
   "source": [
    "We have include an additional argument to see how the evaluation changes over time. It must be said that here we are not talking about perfomance but declaration and the way we think in the model. Both models, procedural and this one are perfectly fine. As final point in the pipelines, it should be mentioned that we can graphicaly explore the pipeline when it is made based on pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f44053b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div class=\"river-component river-pipeline\"><div class=\"river-component river-union\"><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">get_date_features</pre></summary><code class=\"river-estimator-params\">\n",
       "def get_date_features(x):\n",
       "    weekday =  x['date'].weekday()\n",
       "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">y_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">(\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=7\n",
       "  )\n",
       "  target_name=\"y\"\n",
       ")\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">y_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">(\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=14\n",
       "  )\n",
       "  target_name=\"y\"\n",
       ")\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">y_mean_by_store_id</pre></summary><code class=\"river-estimator-params\">(\n",
       "  by=['store_id']\n",
       "  how=Rolling (\n",
       "    obj=Mean ()\n",
       "    window_size=21\n",
       "  )\n",
       "  target_name=\"y\"\n",
       ")\n",
       "\n",
       "</code></details></div><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">~['area_name', 'date', 'genre_name', 'latitude', 'longitude', 'store_id']</pre></summary><code class=\"river-estimator-params\">(\n",
       "  area_name\n",
       "  date\n",
       "  genre_name\n",
       "  latitude\n",
       "  longitude\n",
       "  store_id\n",
       ")\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">StandardScaler</pre></summary><code class=\"river-estimator-params\">(\n",
       "  with_std=True\n",
       ")\n",
       "\n",
       "</code></details><details class=\"river-component river-estimator\"><summary class=\"river-summary\"><pre class=\"river-estimator-name\">LinearRegression</pre></summary><code class=\"river-estimator-params\">(\n",
       "  optimizer=SGD (\n",
       "    lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "  )\n",
       "  loss=Squared ()\n",
       "  l2=0.\n",
       "  l1=0.\n",
       "  intercept_init=0.\n",
       "  intercept_lr=Constant (\n",
       "    learning_rate=0.01\n",
       "  )\n",
       "  clip_gradient=1e+12\n",
       "  initializer=Zeros ()\n",
       ")\n",
       "\n",
       "</code></details></div><style scoped>\n",
       ".river-estimator {\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-pipeline {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    background: linear-gradient(#000, #000) no-repeat center / 3px 100%;\n",
       "}\n",
       "\n",
       ".river-union {\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper {\n",
       "    display: flex;\n",
       "    flex-direction: column;\n",
       "    align-items: center;\n",
       "    justify-content: center;\n",
       "    padding: 1em;\n",
       "    border-style: solid;\n",
       "    background: white;\n",
       "}\n",
       "\n",
       ".river-wrapper > .river-estimator {\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "/* Vertical spacing between steps */\n",
       "\n",
       ".river-component + .river-component {\n",
       "    margin-top: 2em;\n",
       "}\n",
       "\n",
       ".river-union > .river-estimator {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".river-union > .pipeline {\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       "/* Spacing within a union of estimators */\n",
       "\n",
       ".river-union > .river-component + .river-component {\n",
       "    margin-left: 1em;\n",
       "}\n",
       "\n",
       "/* Typography */\n",
       "\n",
       ".river-estimator-params {\n",
       "    display: block;\n",
       "    white-space: pre-wrap;\n",
       "    font-size: 120%;\n",
       "    margin-bottom: -1em;\n",
       "}\n",
       "\n",
       ".river-estimator > .river-estimator-params,\n",
       ".river-wrapper > .river-details > river-estimator-params {\n",
       "    background-color: white !important;\n",
       "}\n",
       "\n",
       ".river-estimator-name {\n",
       "    display: inline;\n",
       "    margin: 0;\n",
       "    font-size: 130%;\n",
       "}\n",
       "\n",
       "/* Toggle */\n",
       "\n",
       ".river-summary {\n",
       "    display: flex;\n",
       "    align-items:center;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".river-summary > div {\n",
       "    width: 100%;\n",
       "}\n",
       "</style></div>"
      ],
      "text/plain": [
       "Pipeline (\n",
       "  TransformerUnion (\n",
       "    FuncTransformer (\n",
       "      func=\"get_date_features\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=7\n",
       "      )\n",
       "      target_name=\"y\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=14\n",
       "      )\n",
       "      target_name=\"y\"\n",
       "    ),\n",
       "    TargetAgg (\n",
       "      by=['store_id']\n",
       "      how=Rolling (\n",
       "        obj=Mean ()\n",
       "        window_size=21\n",
       "      )\n",
       "      target_name=\"y\"\n",
       "    )\n",
       "  ),\n",
       "  Discard (\n",
       "    area_name\n",
       "    date\n",
       "    genre_name\n",
       "    latitude\n",
       "    longitude\n",
       "    store_id\n",
       "  ),\n",
       "  StandardScaler (\n",
       "    with_std=True\n",
       "  ),\n",
       "  LinearRegression (\n",
       "    optimizer=SGD (\n",
       "      lr=Constant (\n",
       "        learning_rate=0.01\n",
       "      )\n",
       "    )\n",
       "    loss=Squared ()\n",
       "    l2=0.\n",
       "    l1=0.\n",
       "    intercept_init=0.\n",
       "    intercept_lr=Constant (\n",
       "      learning_rate=0.01\n",
       "    )\n",
       "    clip_gradient=1e+12\n",
       "    initializer=Zeros ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0354f",
   "metadata": {},
   "source": [
    "One important additional point to highlight is the fact that, in order to debug the behavior of the different steps the function `debug_one`can be used. IMagine that,, in this case we train the model with the firs 120,000 examples and we want to know what happens with the following one. The next piece of code made the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a1ccdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>. Input\n",
       "--------\n",
       "area_name: Tōkyō-to Nerima-ku Toyotamakita <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "date: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00:00</span> <span style=\"font-weight: bold\">(</span>datetime<span style=\"font-weight: bold\">)</span>\n",
       "genre_name: Izakaya <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "is_holiday: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "latitude: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.73562</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "longitude: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139.65166</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "store_id: air_04341b588bde96cd <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Transformer union\n",
       "--------------------\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span> get_date_features\n",
       "    ---------------------\n",
       "    is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "    weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1</span> TargetAgg\n",
       "    -------------\n",
       "    y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2</span> TargetAgg1\n",
       "    --------------\n",
       "    y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.07143</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3</span> TargetAgg2\n",
       "    --------------\n",
       "    y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.23810</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. ~<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'area_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'genre_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'store_id'</span><span style=\"font-weight: bold\">]</span>\n",
       "----------------------------------------------------------------------------\n",
       "is_weekend: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> <span style=\"font-weight: bold\">(</span>bool<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span>\n",
       "y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.28571</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. StandardScaler\n",
       "-----------------\n",
       "is_weekend: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.61171</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "weekday: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.51168</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "y_mean_by_store_id: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28498</span> <span style=\"font-weight: bold\">(</span>float<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. LinearRegression\n",
       "-------------------\n",
       "Name                 Value      Weight      Contribution  \n",
       "         Intercept    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00000</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.75599</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.75599</span>  \n",
       "y_mean_by_store_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28498</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.43069</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.97322</span>  \n",
       "           weekday    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.51168</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.59623</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.86345</span>  \n",
       "        is_weekend   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.61171</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.58198</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.96772</span>  \n",
       "\n",
       "Prediction: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.56038</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m. Input\n",
       "--------\n",
       "area_name: Tōkyō-to Nerima-ku Toyotamakita \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "date: \u001b[1;36m2016\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m00:00:00\u001b[0m \u001b[1m(\u001b[0mdatetime\u001b[1m)\u001b[0m\n",
       "genre_name: Izakaya \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "is_holiday: \u001b[3;92mTrue\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "latitude: \u001b[1;36m35.73562\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "longitude: \u001b[1;36m139.65166\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "store_id: air_04341b588bde96cd \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Transformer union\n",
       "--------------------\n",
       "    \u001b[1;36m1.0\u001b[0m get_date_features\n",
       "    ---------------------\n",
       "    is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "    weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.1\u001b[0m TargetAgg\n",
       "    -------------\n",
       "    y_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.2\u001b[0m TargetAgg1\n",
       "    --------------\n",
       "    y_mean_by_store_id: \u001b[1;36m36.07143\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;36m1.3\u001b[0m TargetAgg2\n",
       "    --------------\n",
       "    y_mean_by_store_id: \u001b[1;36m36.23810\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "y_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. ~\u001b[1m[\u001b[0m\u001b[32m'area_name'\u001b[0m, \u001b[32m'date'\u001b[0m, \u001b[32m'genre_name'\u001b[0m, \u001b[32m'latitude'\u001b[0m, \u001b[32m'longitude'\u001b[0m, \u001b[32m'store_id'\u001b[0m\u001b[1m]\u001b[0m\n",
       "----------------------------------------------------------------------------\n",
       "is_weekend: \u001b[3;91mFalse\u001b[0m \u001b[1m(\u001b[0mbool\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m\n",
       "y_mean_by_store_id: \u001b[1;36m36.28571\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. StandardScaler\n",
       "-----------------\n",
       "is_weekend: \u001b[1;36m-0.61171\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "weekday: \u001b[1;36m0.51168\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "y_mean_by_store_id: \u001b[1;36m1.28498\u001b[0m \u001b[1m(\u001b[0mfloat\u001b[1m)\u001b[0m\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. LinearRegression\n",
       "-------------------\n",
       "Name                 Value      Weight      Contribution  \n",
       "         Intercept    \u001b[1;36m1.00000\u001b[0m    \u001b[1;36m18.75599\u001b[0m       \u001b[1;36m18.75599\u001b[0m  \n",
       "y_mean_by_store_id    \u001b[1;36m1.28498\u001b[0m    \u001b[1;36m12.43069\u001b[0m       \u001b[1;36m15.97322\u001b[0m  \n",
       "           weekday    \u001b[1;36m0.51168\u001b[0m     \u001b[1;36m5.59623\u001b[0m        \u001b[1;36m2.86345\u001b[0m  \n",
       "        is_weekend   \u001b[1;36m-0.61171\u001b[0m    \u001b[1;36m-1.58198\u001b[0m        \u001b[1;36m0.96772\u001b[0m  \n",
       "\n",
       "Prediction: \u001b[1;36m38.56038\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=utils.Rolling(stats.Mean(), n))\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "for x, y in itertools.islice(data, 120_000):\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "x, y = next(iter(data))\n",
    "print(model.debug_one(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5187b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
